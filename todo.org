Updates


* <2023-08-23 Wed 18:50>
It's so over. :DDDDD

** TODO Double check you've addressed all the dumbass comments
Mostly
** TODO Generate all plots with a single executable

** DONE Create code to run the shit with the bekker model
Yes.
*** DONE Create a derived bekker model class to inherit all the
functionality of the HybridDynamics class
*** DONE create a new system class and train it.
*** DONE Remove all use of HybridDynamics from Trainer
to make it abstract.
Working on it.
*** DONE Achieve some dependency injection by using
a factory class to create VehicleSystem objects.
*** DONE Run this shit, see what happens
Got all executables running after the refactor.
Need to make bekker model train/evaluate.
*** DONE Bekker model isn't that great
Should have higher performance.
Try messing with the tire settling hacks.
It seems like it isn't turning enough.
Try different hacks for avoiding divide by zero in the slip_ratio
*** DONE Bekker model isn't turning. wtf
*** TODO Bekker Model error is still higher than it should be.
wtf why.  I dont get it
** Templatize all code for float and ADF
Oh hell nah
** TODO REWRITE IT
the paper


* <2023-08-06 Sun 23:20>
More parameters didn't help, relu didn't help, adding feedback
to the tire network didn't help either.

I think I could write this paper by comparing the trained neural
performance against a 3d model with the Bekker tire-soil model
instead. I think that way I could show an improvement.
I really have no ideas left to try. 

** Discovery: Training with batch size = 1 leads to lower loss
Alright lets try it.
It was trash.
600 steps:
- Training Loss: 2.89%
- CV3: 13.7%
- LD3: 4.205%


** DONE You could delete the front of the test trajectories
There is a short period of time where each trajectory is basically
motionless. So you could delete that shit.

** DONE Hack the sign of Fy
You're seeing a lateral bias when moving forward. Can't be
good for validation loss.


** DONE Different architectures
0 hidden layers (linear)
Trained fast. Was trash.
600 steps:
- Training Loss: like 4% I think
- CV3: Bad
- LD3: Bad

1 hidden layer was trash. It took very long to train.
600 steps:
- Training Loss: 3.1%
- CV3: 14.53%
- LD3: 5.22%

2 hidden layers is normal.
I routinely achieve 9.8% CV3 performance.

3 hidden layers. batch_size=1
600 steps:
- Training Loss: 2.757%
- CV3: 11.90%
- LD3: 4.04 %


* <2023-08-02 Wed 21:00>
Looks like more params aren't going to do it.
I could try training the size 8 model a lot longer. Or try training
the size 16 model with 1 thread, and update params after every test
instead of a full batch size.

* <2023-07-30 Sun 11:46>
Reduce error.
Okay yeah but how
Well the ideas below seemed to work pretty good.
Also you were able to replicate the linear model performance,
so there's really no question of bugs in your code that are
sabotaging performance.

So, where do we go from here? Not sure.
** DONE Try with more parameters?
I'll be honest, I don't think this has ever helped.
But it's easy to test out.
Bruh moment.

600 steps:
- Training Loss:  5.87% (probably could go lower)
- CV3: 15.1%
- LD3: 6.5%

Training Loss: 4.3% and falling fast (slow)
4.2% now, like hours later.
3.85% now
3.55% now.
3.33 plataued 2 days later.
With batched training it seems to be going lower and faster.
3.1% Now 8 hours later.

The last CV3 value was like 12.4% though.
Trash. But maybe it will improve if I train it more.
I'm seeing a trend here where CV3 loss = 3x training loss

Alright fuck it. Adding more parameters hasn't helped. Still around
12% CV3 error. RIP.

** DONE Tire model feedback
Add some state to this. So it's a neural ODE. Oh boy.

f([x,s]) -> [dx,ds]
s is the state. This is a neural ode.
x is the normal tire-model inputs

600 steps:
- Training loss: 2.8%
- CV3: 12.6%
- LD3: 4.2%

Okay not terrible. The very low training loss is cool.
WHen I plot the hidden state it seems to correlate
with the vehicle's yaw which is interesting.

It plataued around 2.78% and I couldnt make the loss go lower.

** DONE Relu vs Tanh
I haven't done this in a while so why not
pretraining loss is high. 29N vs like .5N for tanh.
I am actually feeling really good about this one.
Tanh limits the output values and reduces the expressiveness
of the network which is cringe. I was training at 100 steps
and the loss was going down linearly, at 2.8% and still dropping
.2% each iteration. Fire.

It makes sense that the tanh would not be able to achieve absolute
zero loss. Because the output of such a network is bounded by the
values in the last layer. So even if the inputs reach a very high
value, the output layer will not be proportionally large. For
that reason, I think using tanh is only good when you know the
output of the network must belong to a compact set. But for normal
regression tasks, it seems like ReLU is vastly superior because
the output is unbounded.

WTF
Training with 600 steps,
Here was the training loss per iteration:
Avg Loss: 0.150749
Avg Loss: 0.0961711
Avg Loss: 0.0650992
Avg Loss: 0.0470632
Avg Loss: 0.0364782
Avg Loss: 0.0301677
Avg Loss: 0.0263205

It reached a lower loss than tanh, and exponentially faster.
Mind blown. I can't believe it. This shit is gonna win it for me
I think. Oh my god I think this is it we're so back.

Avg Loss: .77%
It was squared error *Dead*
With L1 loss it plataeus around 3.98%. Rip.

It's so over.


** DONE Check the orientation of the vehicle
Old Branch: task/new_network
New Branch: task/rollpitch
Maybe the ground wasn't so flat. There is some pitch and roll info.
It's definitely non zero. The pitch and roll goes between like -10 and +10 degrees
Very small slope but possibly this info would improve performance if fed
into the tire network somehow.

This would sort of be a hack because ideally we would have a map that represents
the height of the terrain and that info would be used to change the orientation of
the reaction forces which would account for the change in orientation.
Instead of manually hacking the orientation into the tire-model it should result
naturally from the dynamics. That was kind of the whole point.

But also, its a fact that I have no altitude information with this dataset.
1st option: Utilize the video from the dataset to create point clouds and build an
            elevation map (difficult)
2nd option: Cheat. Send roll and pitch to the tire networks.
            Perhaps you would want to do this even if you had a full 3D elevation map.

Maybe this isn't a hack. I could probably justify it.
If this doesnt help though you're fucked	    
This is a dumb stupid idea and I'm not gonna do it right now. 
My main problem is that supplying the model with the ground truth
of roll and pitch during the training process feels like cheating.
But maybe if I just compute roll and pitch from the state of the
vehicle it will help anyways even if it's not the ground truth.
Let me plot the roll and pitch during a normal evaluation of CV3.

During a normal evaluation, the roll was basically zero.
Okay so I am not going to add the roll or pitch to the tire network.

** DONE Better cpt_pt detection
You have some other code for better cpt_detection that actually returns
a valid cpt_rot representing the actual orientation of the contact point
with respect to the tire. This could be as easy as a copy and paste.
One issue is that the cpt detection could really slow things down.
And it also might not be continuous. I think it would actually break
the continuity of the entire model.

Might break the continuity of the model, but it's easy to test. It was
just a copy and paste. No big deal really.

It's going slower but it seems to be training fine.
The loss started higher and now its moving down.

600 steps:
- Plataued arond training Loss:  5.0%
- CV3: 14.1%
- LD3: who cares

Conclusion: no bueno and makes it run slower, achieves a higher training loss. Cringe

* <2023-07-23 Sun 18:03>
New idea bro. Train a new network with no bias neurons, that way
f(x=0) = 0.
Add a penalty promote passivity. penalty = ReLU(dot(f(x), x))
Let's see how this shit goes

Seems to get better performance when training exclusively
on CV3 test 1. 

** DONE Evaluate this idea:
Loss is not monotically decreasing, uh oh. Training is not going
good, oh no. Training loss is stagnating around .20.
Before it would get down to like .08. CV3 loss is terrible.

Try the penalty to promote passivity. Okay that didnt help.
Rip. Lets add the gating back. Rip.

Not bad actually with the gating. Got down to 11% CV3
and 5% on LD3. Which I think is better than the normal model.

** DONE Generate new pretraining data
with a constant zr. Check to see what the zr should be.
Retrain. The bekker model actually did really good. So lets
see if we can replicate that success with a neural net
with just pretraining. Did this. Achieved mega low pretraining
loss. Evaluation loss is .88N which is the best I've seen.
When I was using the sign correction haxx, it was usually like
13.0N

And now 100 step training loss is approaching .08 which is pretty
good, and it doesn't seem to have plataued yet so thats sick.
Pretty good day. Good vibes bruh.

Plataued around 6.88% training loss
12.28% on CV3 (meh)
5.4% on LD3 (not bad)

** DONE Try different numbers of training steps
So you saw decent results with 100 steps. Lets try with more
steps, like 200 and see how that goes. Hopefully the training
process stays stable again.

200 steps:
- Training Loss: 4.75%
- CV3: 11.85%
- LD3: 4.9%

400 steps:
- Training Loss: 3.58%
- CV3: 11.47%
- LD3: 4.74%

600 steps:
- Training Loss: 3.0%
- CV3: 9.58%
- LD3: 4.56%

This is pretty good damn.

** DONE Try training the linear model with the same code as your neural model
Will help me seeing if I'm doing anything wrong.
Also will help in creating a single executable that can generate
all my figures for a paper.

I can't replicate the linear model error from the paper.
Trying to preprocess things differently might help.
If it improves the linear model performance, it might also
improve the neural model performance.

Interpolate velocity in body coordinates. I think this helps.

Changes:
Changed preprocessing to interpolate in velocity in body coordinates
Changed preprocessing to compute wz from yaw instead of the IMU data
        ^For the linear model, this greatly improved LD3 performance

Succesfully brought the evalute_lin_model.py and LinearTrainer.cpp
into pretty close agreement.

** DONE Re run the neural model training with the new preprocessing
Now that you changed the preprocessing to make the 2 linear models
match, maybe the neural model will train better. Idk.
600 steps:
- Training Loss: 2.99%
- CV3 Loss: 9.59%
- LD3 Loss: 4.56%
Not much of an improvement
  
* <2023-07-19 Wed 22:42>
** DONE NEW NETWORK
You had an idea, get rid of that stupid symmetry crap.
Train a network with outputs [Fx,Fy]. And gate the output
of this network like: sigmoid(qd)*[Fx,Fy]. This allows the
network to express really stupid and nonphysical mappings
that might help when turning in place or whatever.

It solves the problem of being 0 at 0
Okay, but then it could slide frictionlessly if qd=0
which would be really stupid.

Could gate it with sigmoid(|qd|+|vx|+|vy|) so that it's forced
to produce only a small output when the overall energy of the
system is low.

You are not even going to evaluate this stupid ass idea

* <2023-07-15 Sat 11:49>
Possible bug, but when you set vl and vr to 0mps you see a drift
in the y direction. Wack. Yep I am seeing a lateral drift. unit test time.
Unit test graph is showing that fy=0. I dont know why.
Where the fuck do bugs come from?
How do they show up inexplicably after a long time of good performance?
I think I've been seeing this for a long time but I've just been ignoring it.
It's a minor bug. it's because you use ReLU on the final layer.
The input to the Fy's ReLU was negative, resulting in 0 fy
force. This is sort of a bug because it means the network can learn
a mapping that makes no sense.
When trained this problem would mostly go away or just become
invisible. But its still possible that for some combo of
vx,vy,zr,w Fy would get a negative value before the relu resulting
in 0 Fy force. Which would probably be overall bad for generalization.

There is still some longitudinal drift. Very small amount.
4cm over 100s.
I think this is jsut because this version of the code, doesn't
iterate over the tire to find the contact point to make sure
that tire forces are normal to the surface. So the vehicle is
pitched very slightly forward and the z forces are microscopically
thrusting it forward.

You spent a lot of time tracking down what you thought were
errors and putting them into unit tests. You didn't find any new
bugs.

Lets replace running loss with just a terminal loss. Idk.

* DONE Convert ReLU to abs
Lets get those results.
So, this might improve generalization.
I think abs makes more sense than relu.
Not sure if this really has any significant effect.
CV3 error is 11.8%. Not sure that this is better. main_8 has been
performing like shit recently anyways so I can't compare.

* DONE Separate Fx,Fy,Fz into separate networks.
There's really no reason they have to be all connected right?
And really, at these low speeds, I would expect that Fy
is really only influenced by vy. And really how could vy affect
vx? Makes no sense.

But also, I dont think it should matter if networks are separated.
You've done this for normal feedforward networks and there's no
difference.

Well this is all I got. Might as well try.
IMPLEMENTED AHHHHHHHHHHH
BUGG AHHHHHHHH
FIXED AHHHHHHHH

Okay bro. I don't have a lot to go on from here. This could be it.
This could be the end.


* DONE Only other idea is to also use the other training data
THe data collected on floors indoor or whatever instead of the ground.
Idk it might help.
Lets add more training data from the other datasets. THere's tile and asphalt.
CV3 performance is trash 13%
I can try training more and going for a smaller training loss idk.
No improvement. Still trash at CV3 13%

* DONE I cant consistently replicate the 8% on CV3
Can't replicate it at all as a matter of fact
No idea why. Maybe I'm just not being patient enough with running
the training over night. I have no idea.

Best I can do is 10.2% on CV3 take it or leave it

* The issue isn't the training loss.
You can bring the training loss down to 5% no problem.
That would be great on CV3 and LD3.

* Its just not generalizing or the model isn't suited to the test data
I'm trying to just train the first trajectory of the CV3 set.
Just to see how low I can make that error go.
CV3 test 1 plataued at like 37.5%. We need to understand why.
plot it.

* <2023-07-11 Tue 22:22>
The fact that we cant bring training error to absolute zero might just be
caused by noise in the training data that is just not model-able by
the model.

Lets train specifically on the first 6 second trajectory in the training data.
So we are only optimizing one specific thing and we should be able to drive that
error to absolute zero.
So yeah, when I train like this, and also mess around with the
learning rate by setting it alternating it high and low, then
I can achieve a lower training loss very close to 0.

At first it plataued at .04, but after jiggling the learning rate,
I was able to get it down to .006
Which is far lower than anything I've seen before.
Just for fun, I ran this "trained" model on CV3
CV3 Error was: 88% lol

So actually, you can bring the training loss very low by
oscillating the learning rate. And a slightly too high learning
rate is not that big of a deal.

* Result:
separating out the z network didn't help. Rip.

* Residual Learning
This is pretty overpowered, but sort of stupid and it reduces explainability.
I think this would probably improve performance.
Residual network could map [vl,vr,vx,vy,wz]->[dvx, dvy, dwz]

* DONE Do a long train over night to evaluate the separate_z thing
Idk if I want to incorporate this change into main_8 yet.
It seems like the separate z thing is making performance worse.
Separate z network seems like it isn't working very well. Riperoni

Then do this:

* DONE Add more to the loss function?
Like velocity?
It is a running loss so the velocity might help.
Adding the yaw error helped.

Bruh. In the process of implementing this idea, you found out
that the gt_vec is expressed in a different frame than the model.
So that was introducing error at the start of every simulation.
Which is terrible. Bruh moment.

as a result of the fix, training loss is getting lower than
ever before. Actually, before fix, I have no idea how training
with a 20 steps would even be successful at all.


* DONE set COM to origin.
Idk it might help a bit to kill the asymetry.

* Add the smooth ground data to the training set???
It might help it learn the dynamics. Bro I dont know.
Terrible idea.

* <2023-06-29 Thu 00:29>
Okay. You have 2 paths forward at this point.
THe main issue is that the training loss is not actually
going very low.

Possible solutions:

* DONE Better optimizer?
Maybe. Possibly getting stuck in local minima.
Lets run main_8 with a large step size all night and see where we
get. Nah. It's just no bueno.

* augmented tire-network neural ode?
How likely is this to actually work? bro I don't feel great about it.
How will you initialize the value? This could be a shit ton of work. Damn.
Feedback has actually not worked out very well for me in the past. I don't
feel very good about it.

Feedback. Gives more params and can reason about changes in the
tire's velocity. Let's try this next.
Add an additional output to the tire network in pretraining.
Then in C++ modify the network
Then add it to the entire state of the fucking vehicle in HybridDynamics
Feed part of the state back into the network.

* DONE Base network?
Could work. Seems like cheating.
In progress.
Forward pass implemented I guess.
todo: replace cpt_vels with temp_vel (minor thing)

It compiled.
Training it over night.
Loss is still fucking plataueing

Final Train3_1 evaluation loss: .108588
Not great tbh. Not good at all really.

Maybe the loss is plataueing due to initial conditions being
slightly off?

* DONE Kill the asymmetry??
Bro I don't know.


* <2023-04-02 Sun 00:04>
I trained a new network, it uses relu, tanh, and sigmoid to make sure everything
crosses zero at zero and everything is continuous. Based.
I created unit tests to visualize the C++ implementation using matplotlib in cpp.
It matches the python plots, at least visually, I didn't actually compare the
numbers lol I just eyeballed it but it's probably good.
I'm surprised "eyeballing it" has an application in programming
* <2023-04-08 Sat 14:00>
I retrained a network to elimiate division.
So slip ratio is just velocity difference,
slip angle is just vy
It reaches the same level of training loss as with slip ratios.
More importantly, it explodes less. Only on a few rare occasions

does any element of the gradient exceed 1000.
Improved slip ratio network by making the epsilon much smaller
Also clamped the slip ratio.
This seems to mitigate most explosions. Only seems to explode at the
beginnings of a run when tire velocities are small

train.evaluate_cv3();
train.evaluate_ld3();

Before training:
CV3 avg loss: 0.0781642
LD3 avg loss: 0.0264296
After training:
CV3 avg loss: 0.0781142
LD3 avg loss: 0.0264126

Very minor improvement.
* <2023-04-09 Sun 16:02>
I retrained, using all available parameters and used RMSprop
instead of straight gradient descent.

Before training:
CV3 avg loss: 0.0781642
LD3 avg loss: 0.0264296
After Training:
CV3 avg loss: 0.0343975
LD3 avg loss: 0.015283

This improvement is Significant. We're so back.
* <2023-04-11 Tue 00:45>
I realized, the ode is still exploding with longer trajectories.
Explodes often with 4s and not at all with 2s.
I should stop fucking around.
Next step is to train with a bigger network, no bekker params,
and using the diff model. This has the highest chance to succeed.
I should run this shit and train the fuck out of it and then
move on with my god damn life.

Model is still training with 4s trajectories, but its slower and
its also exploding pretty often

* <2023-04-16 Sun 12:57>
Not able to get below 10% error.
I basically have no plan at this point, which is bad.
Using a smaller network. Only 8 hidden neurons.
I'm retraining with with 10 timesteps.
Then I'm going to retrain with 60 timesteps.
I'm desparate to get this bitch below 10%

A possible way forward: Make a 2d neural ode model.
Present results for both 3d and 2d models.
Use the 2d model to get the best possible results.
This is just cope. There's still no guarantee of reaching
a good accuracy level. It will maybe work.

Possibly remove the sign correction stuff and add a big
penalty to the loss function for an increasing system energy?
I think this is actually my best bet. I don't want to create a
fancy 2d model. That's pure copium.
Wait fuck, how do I create a loss function for the energy of the
system if the tires are adding energy?
Not sure. Maybe add a loss function to penalize the tire network
for violating energy?

Maybe differentiate through the vanilla bekker model to find
better tire-soil parameters? Have to cope with the slip ratios.
Numerical explosions likely. Bad.

Do I even need conservation of energy haxx now that I made
everything lipshitz continuous? Can I just slap my neural
network boy right in there and call it a day? Maybe I could
add an additional penalty to the loss function for when the
tire network doesn't cross zero at zero? Might work, who knows.

It's probably also worth a shot to just train the bekker params
with a neural tire-soil model.

1. Try to train without sign haxx
2. Try to train just no_slip model but just the bekker params

Currently training a small network at 10 timesteps, then I'm going
to scale it up to like 60 timesteps or more and see if the training
improves.

This is because I tried training with a bigger network and it
reached a worse asymptotic performance so maybe smaller is better.
Current validation loss (at 60 steps) is right about 10%
So maybe retraining over short trajectories with a smaller model was a good idea?
Cool, just realized I forgot to change the number of hidden units in TireNetwork.h
so it was reading in tons of uninitialized data. Wowee. Or was it loading a 20neuron
network from tire.net? World may never know tbh. Fuck. Well now it should train way faster
at least. Shit. Goind to make debug first this time.

Alright sick wow, now that I have this small network properly working it's giving me 10% error
out of the box with no fine tuning nice. Hopefully this shit works out this time.
*Withered wojak meme*
It was evaluating over 10 timesteps. It's over.
I fucked up again and it was evaling over 10 timesteps. Shit, fuck.
Okay, so now we are getting a relative error of 7.94%. Nice. That's enough.
After 1 training update, error blew back up to 15%. WTF
I believe this is the result of gradients that are too big
and I need to clip these gradients. If I wake up in the morning and
my shit is fucked, I'm going to compute stats on the gradients
and figure out a good clipping value.

Training over 10 steps worked great.
Training over 60 steps was terrible.
Validation Loss steadily increased the whole time from 15%->20%
Will try to compute gradient clipping values

Not working. loss is consistently rising. Fuck.

* <2023-04-18 Tue 18:03>
Will try training with no sign haxx and see what happens
Didn't work. Didn't settle. Basically that was expected.
Will try training the bekker params.
Also could try training the shit out of the model with
2 tiemsteps. Idk.

* <2023-04-19 Wed 17:58>
Training with bekker params straight up did not work.
I tried with 2 timesteps and it did much worse.
Maybe it will work for longer timesteps???
I thought I triwed that though?
Nope, training with bekker params is a failure rip.

* <2023-04-21 Fri 12:57>
Honestly training with 10 timesteps or 2 timesteps on the
small network is good enough performance. I think I will be
able to justify it, and make an argument about the difference between
training and test datasets.

* <2023-05-10 Wed 21:56>
Trying to improve performance on the ld3 test dataset.
I am trying to train with 60 steps and changing it from incrementing
by 60 to incrementing by 4 so effectively it trains over the
data many more times. Did not improve significantly.

justin@SenseNet:~/code/auvsl_dynamics_bptt/build$ grep "CV3 avg" train_output.txt
CV3 avg loss: 0.209038
CV3 avg loss: 0.149164
CV3 avg loss: 0.135871
CV3 avg loss: 0.131235
CV3 avg loss: 0.130213
CV3 avg loss: 0.128472
CV3 avg loss: 0.133146
CV3 avg loss: 0.133035
CV3 avg loss: 0.133578
CV3 avg loss: 0.135477
CV3 avg loss: 0.137866
CV3 avg loss: 0.139977
CV3 avg loss: 0.141098
justin@SenseNet:~/code/auvsl_dynamics_bptt/build$ grep "LD3 avg" train_output.txt
LD3 avg loss: 0.152006
LD3 avg loss: 0.151174
LD3 avg loss: 0.151254
LD3 avg loss: 0.151245
LD3 avg loss: 0.15123
LD3 avg loss: 0.151169
LD3 avg loss: 0.151135
LD3 avg loss: 0.151088
LD3 avg loss: 0.151048
LD3 avg loss: 0.151032
LD3 avg loss: 0.150993
LD3 avg loss: 0.150959
LD3 avg loss: 0.150932

* <2023-05-12 Fri 23:50>
Still trying to imrpvoe ld3 test dataset performance.
What if I tried training with timestep = 1e-4? Hmmm, idk lets see
And also, train_steps=2

I don't have any better ideas unfortunately.
LD3 performance is slowly decreasing but it might have
hit a wall. Not sure.
We now come to you live from the training process:
It appears to be slowing down and approaching about .15
Yeah it won't progress past .157. Fuck.

The only way forward that I can think of, is to add angular error
to the loss function. Done lets try it.
Need to fix the preprocessing script to bound yaw values from [-pi,pi]?
Also change loss function to get smallest angle between actual and gt

Adding angular error did not really improve the situation. It did improve
the CV3 accuracy even further but LD3 still craps out at like .155

* <2023-05-19 Fri 23:12>
Fixed possible typo in initializeState. It was:
  xk[14] = gt_state.vx;
  xk[15] = gt_state.vx;

So hopefully now that I changed that 2nd line to vy its
fixed. This should hopefully improve accuracy on LD3.
Spoiler: it didn't.

Another big error in how accuracy was computed.
loss = CppAD::Value(CppAD::sqrt(lin_mse / traj_len)); // (wrong)
vs
loss = CppAD::Value(CppAD::sqrt(lin_mse) / traj_len); // (correct)

This is big. This accounts for the discrepancy in LD3 vs CV3 performance.

CV3 Accuracy was reduced to 3.6%
LD3 Accuracy was reduced to 15.2%x

I don't know why.
I need to actually debug the LD3.
I'm not sure what is actually wrong.
I assumed it was bad longitudinal performance, but it could
literally be anything. I want to understand it it's yaw,x,or y
that is getting fucked up.

* <2023-05-20 Sat 11:45>
SHIT FUCK BITCH. LD3 is improved significantly by fixing these gay retarded errors, but now CV3 sucks.
LD3 avg loss: 0.0682702
CV3 avg loss: 0.140094

Need to evaluate CV3 test performance very carefully and diagnose errors.
It seems like yaw is mostly okay, I think. But longitudinal and lateral
performance is equally fucked. Maybe now that I have the validation
tests fixed, maybe a bigger network will help.

We could help identify possible areas for improvement by checking out the different CV3 trajectories.
There are some that are mostly straight, so we should be able to isolate the longitudinal performance.
Pretty sure.

Okay, so CV3 61 shows an example of us appearing to turn the wrong way.
There's some others. I want to check the initial vx, vy, wz

CV3 73 is pretty high speed (11mps) and the model goes straight
while gt turns. Pretty bad case. Could address this with a better
pretraining dataset.

CV3 98 is completely fucked. Looks like intial conditions are fucked.
CV3 104 is also completely fucked. Looks like intial conditions are fucked.

Im seeing a trend where it looks like for the second interval on each test
trajectory, the initial conditions look fucked.

So, now I'm going to train for a while. The performance should
improve a bit. Then I'm going to evaluate the same tests as above again.

Evaluating the same tests shows great results. Massive improvement.
No more glaring issues. Overall performance on these
4 CV3 tests is like 5.4%

With 32 hidden units:
LD3 avg loss: 1.03729
LD3 avg loss: 1.03623
LD3 avg loss: 0.756829
LD3 avg loss: 0.0846263
LD3 avg loss: 0.0759111
LD3 avg loss: 0.0672367
LD3 avg loss: 0.066986
LD3 avg loss: 0.0688053
LD3 avg loss: 0.0698828
LD3 avg loss: 0.0693427
LD3 avg loss: 0.0685603
LD3 avg loss: 0.0683913
LD3 avg loss: 0.068311

CV3 avg loss: 0.413945
CV3 avg loss: 0.412914
CV3 avg loss: 0.281909
CV3 avg loss: 0.165502
CV3 avg loss: 0.147556
CV3 avg loss: 0.126712
CV3 avg loss: 0.12138
CV3 avg loss: 0.120514
CV3 avg loss: 0.121548
CV3 avg loss: 0.12088
CV3 avg loss: 0.119026
CV3 avg loss: 0.118591
CV3 avg loss: 0.119088

It looks like now that the only main issue on CV3 is that
the relative performance is bad when the trajectory is short.
Rip.

Bad tests are CV3 #6,32,33,34,114 (there are more but hopefully
these are representative of any issues that are present)

Seems like the common thing between all these trajectories
is that the turns are sharp, with the left tires moving
around 0mps or less.

Not sure how to fix this stuff. Oh well.
I've been training with different number of steps.
I tried with 4 steps and it was basically
plataued on the same loss values.

Training now with 60 steps.
Tried with lr=1e-4 and validation loss was just
plateauing/changing very slowly.
Retrying with lr=1e-3 and m_cnt = 20
I can see the parameters are moving a bit more.

After a few hours, we got a 2 CV3 test results
CV3 avg loss: 0.117544
CV3 avg loss: 0.117482

Not significant.

* Ways forward from here (Good ideas are first):
** DONE Add wz to the tire network features.
I think this might improve rotation accuracy with sharp
rotation. This seems the most promising, tbh.
unfortunately, this would also require using a different network
for each tire. This is because, a positive Wz would result in
a different Fx and Fy for each tire. And each tire is getting
the abs of Vx and Vy so it has no idea how to correctly respond
to Wz.

Okay, I did this and got the error down to 10.3% for CV3
and 6.8% for LD3. Still not good enough.

Continued training at 10 steps.
CV3 error is down to 9.64%
ITS HAPPENING OH FUCK
Started training at 30 steps:
CV3 Error sort of stalled around 9.4%
Pretty lame

So now CV3 performance is reaching the linear model's level of
error. But the linear model's LD3 performance is 3.55%.
And our model has 6.3% error on LD3

Forward:
** DONE Add the tanh back and see how performance is affected.
Not as good.
Plateauing at 11.5% which sucks. Getting rid of tanh bought us
1.5% performance. I am literally killing myself for tiny
improvements.

** DONE Next, go up to 16 hidden units
Enough said. Didn't really help. Hmm.
** DONE Experiment with another network for predicting Fx based on Vx.
It has to be separate because the current network only takes diff
and this one needs to take Vx.
** DONE What if you got rid of zr (probably wont do this)
Enough said
CV3 avg loss: 0.1091
CV3 avg loss: 0.109098
CV3 avg loss: 0.109075
CV3 avg loss: 0.108971
CV3 avg loss: 0.108526
CV3 avg loss: 0.107223
CV3 avg loss: 0.105589
CV3 avg loss: 0.104644
CV3 avg loss: 0.104284
CV3 avg loss: 0.104204
CV3 avg loss: 0.104175
CV3 avg loss: 0.103811
CV3 avg loss: 0.103686
CV3 avg loss: 0.103726
CV3 avg loss: 0.10362
CV3 avg loss: 0.103338
CV3 avg loss: 0.103114
CV3 avg loss: 0.103125
CV3 avg loss: 0.103313
CV3 avg loss: 0.103355
CV3 avg loss: 0.103554
CV3 avg loss: 0.103843
CV3 avg loss: 0.104375
CV3 avg loss: 0.105802
CV3 avg loss: 0.107157
CV3 avg loss: 0.10913
CV3 avg loss: 0.110054
CV3 avg loss: 0.112307
This is fucking bullshit.

** (NAH) Experiment with another network for predicting Fx based on Vx.
It has to be separate because the current network only takes diff
and this one needs to take Vx.
** (STUPID) What if you got rid of zr (probably wont do this)
I don't think there is a significant variation in sinkage
for the jackal.
You could have one network that maps zr->sinkage
And then another network that does [Vx,Vy,Wz]->[Fx,Fy]

** DONE Go back to 8 hidden units
Check the accuracy. 32 hidden might not be necessary.
It's going a lot slower which is incovenient.

** DONE Remove the Tanh.
The tanh was used as a soft sign function. I'm not sure it was
a good idea. Using just diff or vy allows the Fx or Fy to
grow as diff or vy grow. Pretraining shows slightly better
accuracy too. Attempting this. right now.

Not much of an effect honestly.
CV3 avg loss: 0.120793
CV3 avg loss: 0.120793
CV3 avg loss: 0.121087

Maybe this would have gotten better if I let it run longer
but I don't think so.

I removed the tanh and went down to 8 hidden units.
We hit a wall at 12.97% CV3 error. So the 32 unit network gets
down to about 12%. Not great.


** DONE Maybe an atan based slip angle was necessary?
Slip angle changing depending on Vx might be necessary
to get better performance at low Vx, which would cause a higher
slip angle.
This slightly improved pretrain performance, but the scatter plots
still show that the error is worse when vx is low.
Not worth exploring imo.

** DONE Try training with Vx instead of diff?
Can check this with pretraining. This informs the network
directly about Vx which could address the above point as well.
Pretraining shows a slightly increased error level.
But this still might translate to a decreased validation test
level. Not really worth exploring.

Big brain thoughts incoming:
** DONE 4 networks, one for each tire.
Unlikely, but perhaps the added parameters will help.
And maybe there is a significant different between tires.
** DONE Only use zr and diff feature. Only return Fz,Fx
Fz is necesssary or we will sink through the floor.
But, the linear model just maps [vl,vr]->[vx,vy,wz]
So in my opinion, this shows that the model is almost completely
kinematic. So maybe simpler is better. Wait no, this is a dumb
idea. It would be able to slide laterally whicih would be bad.

** DONE Forward: modify loss function to include relative error
This will cause the loss function to focus on the small paths
and hopefully it will increase CV3 performance.
CV3 performance is at 8.66%
LD3 is around 6% so not great.
Continuiing training at 60 steps.


** DONE MULTITHREAD FINALLY
You've got 16 cores, use them all.

* A new way Forward: <2023-05-22 Mon 18:07>
Shit is enfuckulated.
I think you need to add the physical parameters to the model. Or, add another network
to apply an external force to the body of the vehicle to give us the extra params needed
to hack this fucking shit. I would prefer to use the physical params.

* DONE Rotate Initial quaternion according to yaw
* DONE Testing C++ code
* DONE Preprocess test data sets
* DONE settle. create initial position.
* DONE Create Unit tests
** unit test for settling, add a plot
** DONE Unit Test to confirm symmetry of the tire network
* DONE Train a New Network
Fuck. How should I architect this network.
Final Layer should be ReLU * Tanh(sign corection)
This enforces the basic rule of friction, that it opposes movement
* DONE Now that we have the network, S I M U L A T E
** DONE Create some unit tests
Create unit tests for basic simulations
Like moving forward along a straight line,
Moving in a circle
beautiful. So smooth and nice
** DONE Experiment with different settling damping hacks
Check the straight line performance with different settling haxx
Didn't see much difference when changing the damping value from
like -200 to -1000
** DONE Nate dogg and Warren G had to S I M U L A T E
So its settling and driving straight in a circle.
Lets evaluate the untrained performance on the test data sets.
* DONE TRAIN NO WORKO
This is bad, because basically it's a brickwall if I can't get
around it somehow. I tried the most basic form of the problem.
I trained one parameter. The loss blew up and the param -> nan.
I trained one parameter and took an average over 10 trajectories. The loss blew up and the param -> nan.
I trained one parameter and took an average over 100 trajectories. The loss blew up and the param -> nan.
1 param, 100 traj, 2s traj, replace floats with double: param->nan
Traino, yes worko :)

So, this is not working because for some rare trajectories, the value of the gradient inexplicably explodes.
* DONE Exclude outlier gradient magnitudes
* DONE Running Loss? Didn't kill gradient explosions
* DONE Try smaller timestep? This actually seems like it works. WTF.
This seems to actually solve the problem fuck. But its too slow.
God damn it. Still some gradient explosions magnitude 1.
* DONE Identify the source of gradient explosions?
It could be that some part of it is not lipshitz, or it could
just be the general gradient variance problem that they talk
about in the paper "Gradients are not all you need"
It's caused by inverses, and division. Basically any
non-lipshitz component.
* DONE Adjust the small constant added to division?
in slip ratio and slip angle.
This fucking worked. It got rid of the 1e18 bullshit
but it still varies from 1e-6 to 1 which is atrocious
* New Network with non-lipshitz components eliminated?
Replace slip angle with Vy, replace slip ratio with vx - tire_tangent_vel
I'm not sure this would solve all the problems
* Colocation method (train derivatives)
Cheating. But simple and apparently works
Alternatively, just use very small trajectory length, I think.
I don't think colocation is going to work here because the real
data is too noisy. I would have to compute target derivatives
using finite differences which would be way too noisy.
* DONE Smaller duration trajectories
No Effect. Even with 2 points (smallest trajectory possible)
The gradient still explodes up to 1e18.
But now I can make a unit test to replicate this behavior and
find the source of it.

* DONE Euler vs RK4?
Idk why not.
Nope still explosive

* DONE Unit test to replicate exploding gradient
Able to replicate, I find it doesn't blow up out of nowhere,
it gradually blows up over a 100 steps.
Able to prevent the blow up by modifying the epsilon used to
avoid divide by zero when computing slip ratio.
Making the machine eps extremely small prevented any gradient
explosions when using train. This is great news. I am overjoyed.
Still getting gradient explosions, but much smaller magnitude.
~|1|
You could still just retrain the network to avoid dividing.

* DONE OH FUCK I WAS RETARDED AND MADE IT disCONTINUOUS OH SHIDDDDD
This will probably not solve the gradient-splosions.
Need to remove the discontinuity where Fz == 0 when zr < 0

* DONE Issues with ratio and diff networks
I noticed the original slip ratio network, occasionally has huge
.cpp training loss
Ratio network with 1e-12 epsilon is not settling correctly. (slip ratio explodes to 1e12)
Diff network has bad behavior (too much turning)
I fixed the ratio network by clamping the slip ratio.
Still seeing occasional massive gradient explosions. Usually at the
beginning of a test when tire velocities are zero



* <2023-05-27 Sat 16:23>
Okay, the plan is to fix multi threading, then add another
network that applies an external force to the robot's base.

Currently, as far as multithreading goes, you've realized you just
need to make sure you copy m_params for each thread because
CppAD is too stupid to operate on the same CppAD::vector in
multiple threads.

Currently doing a test run with multithreading to make sure it
works and can reach the same performance that single threading
does.
Then: Remove quaternion initialization and fix the input scalers.

Okay multithreading completed. What else is needed for the final
run?

First figure out if we should ignore zr.
Then evaluate the benefit of L1 regularization.

1. Multithreading
2. No Zr? (maybe)
3. No initial Quaternion
4. Base Link network. Why not.
   Helps demonstrate my genius idea of articulated body algorithm + nonlinear disturbances on every body
5. Probably need regularization. L1. Super easy to implement.
   

** Okay, but what if we fixed zr?
This dataset is 2d. Fz doesn't really matter, it just has to keep
us from sinking into the ground.

So, how much is the zr noise affecting training? Would it be better
to just ignore that and restrict motion to 2D during training?
And then do fine tuning pass where 3D motion is reactivated?

It would be easy to implement, why not try it?

Actually, why not train and evaluate in 2D?
The other models are 2D, so it would be a fair comparison.
I don't see why not.

** DONE L1 Regularization
I think this could be big.
I think it explains why the training loss is decreasing but the
test loss is shit for larger tire-network sizes.
Seriously, I think the 16 and 32 size networks are just
overfitting and thats why theyre so shitty. Makes sense.

* We're gonna do these tasks:
** DONE Get rid of initial quaternion?
The initial quaternion determined by natural settling into the
ground. It has some close to zero pitch and roll but the yaw
component is .3 degrees. No bueno. Probably should delete.

** DONE THe fucking input scalers are biased
You took the scaled the absolute value of the inputs, not the actual
inputs. Fuck. This causes a non zero bias term. Shid. Will fix this
and see how much it actually affects performance. Pretraining shows
a small improvement in evaluation loss. OKay, yeah you will copy
that shit in.
** DONE Get rid of Wz
It hasn't done anything for me.

* DONE Okay so there is definitely a data race
But fortunately its rare and it doesn't seem to be
causing problems so you can ignore it until it causes a problem.
Here's the state of the worker over time:
1. Initial: Idle is set to true.
2. Main: A trajectory is loaded, then m_ready is set true,
3. Worker: if m_ready, then:
   set m_ready false
   process data
   set m_waiting true
4. Main: if m_waiting, then:
   combine results
   set m_waiting false
   set m_idle true

In both threads, if anything is true, it is then set false.

idle->ready->(all false)->waiting->idle
   
   

* DONE You profiled the multithreaded code
You spent 94% of your time waiting to lock and unlock mutexes
so that was bad you fucking idiot.
So you deleted the mutex. But it turns out you needed that because
of race conditions it can cause a crash when reading and writing
to the g_map_id vector.
2 Solutions:
1. Join all workers after they have been assigned tasks. (simple, slower)
2. Rewrite workers so that they have a while loop and they wait
   for new data to run on instead of exiting. (fancy, faster)

* DONE Bugfix for computeEqState
You fucked up. The eq state is only initialized once in the
constructor. So if you intiialize your params to something
retarded, you will be stuck with a retarded initial state for the
rest of your training. Shit. This could have been having a small
effect on loss. or big idk. Actually this is not a big deal.

* <2023-06-07 Wed>
Main Issue: training loss is low and validation loss is high.
** DONE Try with zr fixed. Idk lets see what happens
Running. Will run a long training thing and investigate the resulting
CV3 performance. Error was weirdly high, so thats not good.
CV3 error is now down to 12%.
Nothing game changing here.
It trains 5x faster because you were able to up the timestep
but thats the only benefit.
The training loss is reaching a flat line and the validation
loss is oscillating around 12-13%

** DONE Implement Regularization.
Simple L1 reg should solve all of my problems.
This brought CV3 error down from ~16% to a minimum of 10.3%
I will see what happens if I apply regularization to the pretraining network.
Nothing really. It doesn't improve performance on the eval dataset.

Let's test L1 regularization with 8 hidden nodes.
Since 8 hidden nodes gave the best results, L1 + 8 Hidden
should be like, really good.

It's reaching 9.18% CV3 accuracy

** If ^ Doesnt improve performance, implement a base network
Not sure if this will help.
Also, it's cheating.
can't be pretrained either.

* <2023-06-10 Sat>
The above changes weren't enough. It improved 16 node performance
down to 11% on CV3. Didn't affect 8 node performance.
I think the generalization is the main issue.
Training loss is amazing.
Validation loss is not great.

** DONE Add Noise to the model inputs
Improve generalization. PLEASE GOD.
Starting with a relatively large amount: 1e-2
Yeah, this did not help at all. CV3 error slowly climbed up
to 9.8%

** DONE Smooth the left and right tire velocities???
These are kinda noisey. Maybe it's throwing off the model.
Bro IDK. Maybe give the model both unfiltered and filtered
tire velocity. Bro, IDK

** DONE Penalize angular errors harder?
Okay, sounds good
Meh.

** DONE Compute Evaluation loss on the training sets.
It's at 8% which is surprising.
So it's not my generalization that is an issue.

** DONE Analyze performance on the LD3 set
No conclusion. SOmetimes the problem is longitudinal error,
lateral error, and sometimes angular error.




* <2023-06-11 Sun 12:20>
So you thought generalization was bad, but the loss function
was actually different from the true evaluation loss.
So it turns out the training loss is actually not that good.
I changed the loss function.

** DONE Modify loss function
This didn't help that much.
Still same performance levels.


** Okay bro, we are gonna have to do the
base network thing and hope it works out.

** DONE Is there any way we can augment the training datasets                                                                                                                  
Could use the linear model to generate circular trajectories                                                                                                                   
to improve CV3 performance. I guess.                                                                                                                                           
                                                                                                                                                                               
Could generate the mirror image of the training data and pretend                                                                                                               
it's valid. This is currently running, I would be shocked if                                                                                                                   
this had a major effect. But I wouldn't mind being pleasantly                                                                                                                  
surprised. Meanwhile, I am implementing python code to generate                                                                                                                
fake data using the linear model.                                                                                                                                              
                                                                                                                                                                               
CV3 error is down to 10.9% and seems like it's still going down                                                                                                                
Training error was at 8.18% and going down.                                                                                                                                    
                                                                                                                                                                               
Training error hit 8.04%, CV3 error was up to 11.85%. RIP.                                                                                                                     
Training error hit about 7.8% and plataued.                                                                                                                                    
CV3 error was increasing around 12.9%. Massive Rip.                                                                                                                            
                                                                                                                                                                               
So I'm seeing a clear trend.                                                                                                                                                   
Training error decreases continuously.                                                                                                                                         
Test error decreases, reaches a minimum, and then increases again.                                                                                                             
                                                                                                                                                                               
                                                                                                                                                                               
I think the linear model will be able to generate good straight                                                                                                                
line data. Which could bring the neural ode error down to                                                                                                                      
competitive levels. Could also hopefully generate good                                                                                                                         
curved path data which could bring CV3 error levels down a lot.                                                                                                                
                                                                                                                                                                               
                                                                                                                                                                               
Bro I'm feeling really good about this.                                                                                                                                        
                                                                                                                                                                               
I generated 19 fake training datasets using the linear model.                                                                                                                  
The neural model shows a significant difference and I'm                                                                                                                        
thinking that it will be able to learn some useful shit                                                                                                                        
from the linear model. I'm actually feeling really                                                                                                                             
good and I think this avenue of research might actually                                                                                                                        
pan out which would be fucking sick.                                                                                                                                           
                                                                                                                                                                               
You should check the test vs the training datasets                                                                                                                             
and see where we are lacking and generate fake data for it.                                                                                                                    
Check the Wz, Vx, Vl, and Vr for the training data                                                                                                                             
against the test data. I'm not sure the training data has                                                                                                                      
any negative Vl/Vr so that could help.                                                                                                                                         
                                                                                                                                                                               
You tried the linear network, you tried the mirrored training                                                                                                                  
data. You saw performance get down to 9.2% CV3 at one point,                                                                                                                   
but I think you've seen it do that before without fake data.                                                                                                                   
                                                                                                                                                                               
* DONE Add batch sizes back in                                                                                                                                                 
Small batch size is supposed to be better than full batch size.                                                                                                                
DOesn't seem to help. If anything its worse.                                                                                                                                   
                                                                                                                                                                               
* DONE Double check zr -> Fz is continuous                                                                                                                                     
I think it's not which is bad.                                                                                                                                                 
It's continuous.

* <2023-06-24 Sat 11:42>
Wow okay now what. Adding fake data did not work. Very close
to running out of shit to try. This is fucked. You could
do the base network bullshit. I guess. I think that's my only
way forward at this point.

* DONE Base Network :(

* DONE Smooth Tire Velocities
Oh boy. Yeah we got to do this.
So send in the current tire velocity and also a moving average of
each tire velocity. Like:
Vl_s = .5*(Vl_s + Vl)

So store smooth tire velocity as a state in VehicleSystem. And add
some way to reset it.

This idea is kinda fucking lame and would require a lot of work.
I could test drive this idea by going into the preprocessing
script and I could apply a low-pass filtering on all the tire
velocities and see what happens when I train with that.

No improvement. After training all night, error reached a minimum
of 9.7%
* I Think the biggest problem is that Training error is not good
Training Evaluation loss is 8.85%
Which is fucked.
Reduce training error.
Just train on the first file to show you can do it.

* DONE Separate Fz and Fx,Fy networks
With 8 nodes:
Train3_1 Eval Loss: 3.33%
Train3_1 Train loss: 1.52%

With 16 nodes:
Probably the same. I stopped at
1.8% training loss. But it was probably reaching the same
asymptote. I jsut didn't want to wait around for it.
Train3_1 Eval loss: 4.8% (undertrained)

Lets try with a fucking ton of hidden units. Like 64. Fuck it.
Training was Very slow. Seemed to slow down around 9% but was
still going down. Just too slow to be practical.

Try more shit. Anything to get the training loss lower.
Ways to get the fucking loss down:
More features.
More parameters (shit).
Different architecture. 

** So I'm trying without the CppAD::abs()
Idk it's taking a while. It stopped around 2.5%

** Tried no Haxx
And it was complete shit. As expected.

** DONE BASE NETWORK FUUUUUUCKKKKKKKK
I didn't want to do this. But I literally see no other way forward.


