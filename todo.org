Updates
* <2023-04-02 Sun 00:04>
I trained a new network, it uses relu, tanh, and sigmoid to make sure everything
crosses zero at zero and everything is continuous. Based.
I created unit tests to visualize the C++ implementation using matplotlib in cpp.
It matches the python plots, at least visually, I didn't actually compare the
numbers lol I just eyeballed it but it's probably good.
I'm surprised "eyeballing it" has an application in programming
* <2023-04-08 Sat 14:00>
I retrained a network to elimiate division.
So slip ratio is just velocity difference,
slip angle is just vy
It reaches the same level of training loss as with slip ratios.
More importantly, it explodes less. Only on a few rare occasions

does any element of the gradient exceed 1000.
Improved slip ratio network by making the epsilon much smaller
Also clamped the slip ratio.
This seems to mitigate most explosions. Only seems to explode at the
beginnings of a run when tire velocities are small

train.evaluate_cv3();
train.evaluate_ld3();

Before training:
CV3 avg loss: 0.0781642
LD3 avg loss: 0.0264296
After training:
CV3 avg loss: 0.0781142
LD3 avg loss: 0.0264126

Very minor improvement.
* <2023-04-09 Sun 16:02>
I retrained, using all available parameters and used RMSprop
instead of straight gradient descent.

Before training:
CV3 avg loss: 0.0781642
LD3 avg loss: 0.0264296
After Training:
CV3 avg loss: 0.0343975
LD3 avg loss: 0.015283

This improvement is Significant. We're so back.
* <2023-04-11 Tue 00:45>
I realized, the ode is still exploding with longer trajectories.
Explodes often with 4s and not at all with 2s.
I should stop fucking around.
Next step is to train with a bigger network, no bekker params,
and using the diff model. This has the highest chance to succeed.
I should run this shit and train the fuck out of it and then
move on with my god damn life.

Model is still training with 4s trajectories, but its slower and
its also exploding pretty often

* <2023-04-16 Sun 12:57>
Not able to get below 10% error.
I basically have no plan at this point, which is bad.
Using a smaller network. Only 8 hidden neurons.
I'm retraining with with 10 timesteps.
Then I'm going to retrain with 60 timesteps.
I'm desparate to get this bitch below 10%

A possible way forward: Make a 2d neural ode model.
Present results for both 3d and 2d models.
Use the 2d model to get the best possible results.
This is just cope. There's still no guarantee of reaching
a good accuracy level. It will maybe work.

Possibly remove the sign correction stuff and add a big
penalty to the loss function for an increasing system energy?
I think this is actually my best bet. I don't want to create a
fancy 2d model. That's pure copium.
Wait fuck, how do I create a loss function for the energy of the
system if the tires are adding energy?
Not sure. Maybe add a loss function to penalize the tire network
for violating energy?

Maybe differentiate through the vanilla bekker model to find
better tire-soil parameters? Have to cope with the slip ratios.
Numerical explosions likely. Bad.

Do I even need conservation of energy haxx now that I made
everything lipshitz continuous? Can I just slap my neural
network boy right in there and call it a day? Maybe I could
add an additional penalty to the loss function for when the
tire network doesn't cross zero at zero? Might work, who knows.

It's probably also worth a shot to just train the bekker params
with a neural tire-soil model.

1. Try to train without sign haxx
2. Try to train just no_slip model but just the bekker params

Currently training a small network at 10 timesteps, then I'm going
to scale it up to like 60 timesteps or more and see if the training
improves.

This is because I tried training with a bigger network and it
reached a worse asymptotic performance so maybe smaller is better.
Current validation loss (at 60 steps) is right about 10%
So maybe retraining over short trajectories with a smaller model was a good idea?
Cool, just realized I forgot to change the number of hidden units in TireNetwork.h
so it was reading in tons of uninitialized data. Wowee. Or was it loading a 20neuron
network from tire.net? World may never know tbh. Fuck. Well now it should train way faster
at least. Shit. Goind to make debug first this time.

Alright sick wow, now that I have this small network properly working it's giving me 10% error
out of the box with no fine tuning nice. Hopefully this shit works out this time.
*Withered wojak meme*
It was evaluating over 10 timesteps. It's over.
I fucked up again and it was evaling over 10 timesteps. Shit, fuck.
Okay, so now we are getting a relative error of 7.94%. Nice. That's enough.
After 1 training update, error blew back up to 15%. WTF
I believe this is the result of gradients that are too big
and I need to clip these gradients. If I wake up in the morning and
my shit is fucked, I'm going to compute stats on the gradients
and figure out a good clipping value.

Training over 10 steps worked great.
Training over 60 steps was terrible.
Validation Loss steadily increased the whole time from 15%->20%
Will try to compute gradient clipping values

Not working. loss is consistently rising. Fuck.

* <2023-04-18 Tue 18:03>
Will try training with no sign haxx and see what happens
Didn't work. Didn't settle. Basically that was expected.
Will try training the bekker params.
Also could try training the shit out of the model with
2 tiemsteps. Idk.

* <2023-04-19 Wed 17:58>
Training with bekker params straight up did not work.
I tried with 2 timesteps and it did much worse.
Maybe it will work for longer timesteps???
I thought I triwed that though?
Nope, training with bekker params is a failure rip.

* <2023-04-21 Fri 12:57>
Honestly training with 10 timesteps or 2 timesteps on the
small network is good enough performance. I think I will be
able to justify it, and make an argument about the difference between
training and test datasets.

* <2023-05-10 Wed 21:56>
Trying to improve performance on the ld3 test dataset.
I am trying to train with 60 steps and changing it from incrementing
by 60 to incrementing by 4 so effectively it trains over the
data many more times. Did not improve significantly.

justin@SenseNet:~/code/auvsl_dynamics_bptt/build$ grep "CV3 avg" train_output.txt
CV3 avg loss: 0.209038
CV3 avg loss: 0.149164
CV3 avg loss: 0.135871
CV3 avg loss: 0.131235
CV3 avg loss: 0.130213
CV3 avg loss: 0.128472
CV3 avg loss: 0.133146
CV3 avg loss: 0.133035
CV3 avg loss: 0.133578
CV3 avg loss: 0.135477
CV3 avg loss: 0.137866
CV3 avg loss: 0.139977
CV3 avg loss: 0.141098
justin@SenseNet:~/code/auvsl_dynamics_bptt/build$ grep "LD3 avg" train_output.txt
LD3 avg loss: 0.152006
LD3 avg loss: 0.151174
LD3 avg loss: 0.151254
LD3 avg loss: 0.151245
LD3 avg loss: 0.15123
LD3 avg loss: 0.151169
LD3 avg loss: 0.151135
LD3 avg loss: 0.151088
LD3 avg loss: 0.151048
LD3 avg loss: 0.151032
LD3 avg loss: 0.150993
LD3 avg loss: 0.150959
LD3 avg loss: 0.150932

* <2023-05-12 Fri 23:50>
Still trying to imrpvoe ld3 test dataset performance.
What if I tried training with timestep = 1e-4? Hmmm, idk lets see
And also, train_steps=2

I don't have any better ideas unfortunately.
LD3 performance is slowly decreasing but it might have
hit a wall. Not sure.
We now come to you live from the training process:
It appears to be slowing down and approaching about .15
Yeah it won't progress past .157. Fuck.

The only way forward that I can think of, is to add angular error
to the loss function. Done lets try it.
Need to fix the preprocessing script to bound yaw values from [-pi,pi]?
Also change loss function to get smallest angle between actual and gt

Adding angular error did not really improve the situation. It did improve
the CV3 accuracy even further but LD3 still craps out at like .155

* <2023-05-19 Fri 23:12>
Fixed possible typo in initializeState. It was:
  xk[14] = gt_state.vx;
  xk[15] = gt_state.vx;

So hopefully now that I changed that 2nd line to vy its
fixed. This should hopefully improve accuracy on LD3.
Spoiler: it didn't.

Another big error in how accuracy was computed.
loss = CppAD::Value(CppAD::sqrt(lin_mse / traj_len)); // (wrong)
vs
loss = CppAD::Value(CppAD::sqrt(lin_mse) / traj_len); // (correct)

This is big. This accounts for the discrepancy in LD3 vs CV3 performance.

CV3 Accuracy was reduced to 3.6%
LD3 Accuracy was reduced to 15.2%x

I don't know why.
I need to actually debug the LD3.
I'm not sure what is actually wrong.
I assumed it was bad longitudinal performance, but it could
literally be anything. I want to understand it it's yaw,x,or y
that is getting fucked up.

* <2023-05-20 Sat 11:45>
SHIT FUCK BITCH. LD3 is improved significantly by fixing these gay retarded errors, but now CV3 sucks.
LD3 avg loss: 0.0682702
CV3 avg loss: 0.140094

Need to evaluate CV3 test performance very carefully and diagnose errors.
It seems like yaw is mostly okay, I think. But longitudinal and lateral
performance is equally fucked. Maybe now that I have the validation
tests fixed, maybe a bigger network will help.

* DONE Rotate Initial quaternion according to yaw
* DONE Testing C++ code
* DONE Preprocess test data sets
* DONE settle. create initial position.
* DONE Create Unit tests
** unit test for settling, add a plot
** DONE Unit Test to confirm symmetry of the tire network
* DONE Train a New Network
Fuck. How should I architect this network.
Final Layer should be ReLU * Tanh(sign corection)
This enforces the basic rule of friction, that it opposes movement
* DONE Now that we have the network, S I M U L A T E
** DONE Create some unit tests
Create unit tests for basic simulations
Like moving forward along a straight line,
Moving in a circle
beautiful. So smooth and nice
** DONE Experiment with different settling damping hacks
Check the straight line performance with different settling haxx
Didn't see much difference when changing the damping value from
like -200 to -1000
** DONE Nate dogg and Warren G had to S I M U L A T E
So its settling and driving straight in a circle.
Lets evaluate the untrained performance on the test data sets.
* DONE TRAIN NO WORKO
This is bad, because basically it's a brickwall if I can't get
around it somehow. I tried the most basic form of the problem.
I trained one parameter. The loss blew up and the param -> nan.
I trained one parameter and took an average over 10 trajectories. The loss blew up and the param -> nan.
I trained one parameter and took an average over 100 trajectories. The loss blew up and the param -> nan.
1 param, 100 traj, 2s traj, replace floats with double: param->nan
Traino, yes worko :)

So, this is not working because for some rare trajectories, the value of the gradient inexplicably explodes.
* DONE Exclude outlier gradient magnitudes
* DONE Running Loss? Didn't kill gradient explosions
* DONE Try smaller timestep? This actually seems like it works. WTF.
This seems to actually solve the problem fuck. But its too slow.
God damn it. Still some gradient explosions magnitude 1.
* DONE Identify the source of gradient explosions?
It could be that some part of it is not lipshitz, or it could
just be the general gradient variance problem that they talk
about in the paper "Gradients are not all you need"
It's caused by inverses, and division. Basically any
non-lipshitz component.
* DONE Adjust the small constant added to division?
in slip ratio and slip angle.
This fucking worked. It got rid of the 1e18 bullshit
but it still varies from 1e-6 to 1 which is atrocious
* New Network with non-lipshitz components eliminated?
Replace slip angle with Vy, replace slip ratio with vx - tire_tangent_vel
I'm not sure this would solve all the problems
* Colocation method (train derivatives)
Cheating. But simple and apparently works
Alternatively, just use very small trajectory length, I think.
I don't think colocation is going to work here because the real
data is too noisy. I would have to compute target derivatives
using finite differences which would be way too noisy.
* DONE Smaller duration trajectories
No Effect. Even with 2 points (smallest trajectory possible)
The gradient still explodes up to 1e18.
But now I can make a unit test to replicate this behavior and
find the source of it.

* DONE Euler vs RK4?
Idk why not.
Nope still explosive

* DONE Unit test to replicate exploding gradient
Able to replicate, I find it doesn't blow up out of nowhere,
it gradually blows up over a 100 steps.
Able to prevent the blow up by modifying the epsilon used to
avoid divide by zero when computing slip ratio.
Making the machine eps extremely small prevented any gradient
explosions when using train. This is great news. I am overjoyed.
Still getting gradient explosions, but much smaller magnitude.
~|1|
You could still just retrain the network to avoid dividing.

* DONE OH FUCK I WAS RETARDED AND MADE IT disCONTINUOUS OH SHIDDDDD
This will probably not solve the gradient-splosions.
Need to remove the discontinuity where Fz == 0 when zr < 0

* DONE Issues with ratio and diff networks
I noticed the original slip ratio network, occasionally has huge
.cpp training loss
Ratio network with 1e-12 epsilon is not settling correctly. (slip ratio explodes to 1e12)
Diff network has bad behavior (too much turning)
I fixed the ratio network by clamping the slip ratio.
Still seeing occasional massive gradient explosions. Usually at the
beginning of a test when tire velocities are zero
