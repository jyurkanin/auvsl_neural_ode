Updates
* <2023-04-02 Sun 00:04>
I trained a new network, it uses relu, tanh, and sigmoid to make sure everything
crosses zero at zero and everything is continuous. Based.
I created unit tests to visualize the C++ implementation using matplotlib in cpp.
It matches the python plots, at least visually, I didn't actually compare the
numbers lol I just eyeballed it but it's probably good.
I'm surprised "eyeballing it" has an application in programming
* <2023-04-08 Sat 14:00>
I retrained a network to elimiate division.
So slip ratio is just velocity difference,
slip angle is just vy
It reaches the same level of training loss as with slip ratios.
More importantly, it explodes less. Only on a few rare occasions

does any element of the gradient exceed 1000.
Improved slip ratio network by making the epsilon much smaller
Also clamped the slip ratio.
This seems to mitigate most explosions. Only seems to explode at the
beginnings of a run when tire velocities are small

train.evaluate_cv3();
train.evaluate_ld3();

Before training:
CV3 avg loss: 0.0781642
LD3 avg loss: 0.0264296
After training:
CV3 avg loss: 0.0781142
LD3 avg loss: 0.0264126

Very minor improvement.
* <2023-04-09 Sun 16:02>
I retrained, using all available parameters and used RMSprop
instead of straight gradient descent.

Before training:
CV3 avg loss: 0.0781642
LD3 avg loss: 0.0264296
After Training:
CV3 avg loss: 0.0343975
LD3 avg loss: 0.015283

This improvement is Significant. We're so back.
* <2023-04-11 Tue 00:45>
I realized, the ode is still exploding with longer trajectories.
Explodes often with 4s and not at all with 2s.
I should stop fucking around.
Next step is to train with a bigger network, no bekker params,
and using the diff model. This has the highest chance to succeed.
I should run this shit and train the fuck out of it and then
move on with my god damn life.

Model is still training with 4s trajectories, but its slower and
its also exploding pretty often
* <2023-04-18 Tue 20:06>
This shit appears to have worked. just training the bekker params.
I'm appalled. God I hope I didn't fuck something up and the
numbers I'm seeing are real. If not, I will be taking my own life.
I double checked and I don't see anything wrong with the math.

The numbers werne't real. The loss function was missing a sqrt.
The real CV3 error is like .127, LD3 is .0745

* <2023-04-19 Wed 23:08>
Okay bro. I'm nearly out of ideas, so I am training on the
bekker params, at 60 steps. I trained with 60 steps and 1e-4
learning rate. But the decrease in loss was very small. I'm goign
to try again with lr=1e-2. This ran overnight and barely trained.
It only improved the loss slightly. I think I need to turn the
gradient explosion checking off.

Gradient explosion turned off. LR=1e-2 did basically nothing but
the loss was at least monotonically decreasing.

Upping the LR to 1.0, fuck it. Not working.





* DONE Rotate Initial quaternion according to yaw
* DONE Testing C++ code
* DONE Preprocess test data sets
* DONE settle. create initial position.
* DONE Create Unit tests
** unit test for settling, add a plot
** DONE Unit Test to confirm symmetry of the tire network
* DONE Train a New Network
Fuck. How should I architect this network.
Final Layer should be ReLU * Tanh(sign corection)
This enforces the basic rule of friction, that it opposes movement
* DONE Now that we have the network, S I M U L A T E
** DONE Create some unit tests
Create unit tests for basic simulations
Like moving forward along a straight line,
Moving in a circle
beautiful. So smooth and nice
** DONE Experiment with different settling damping hacks
Check the straight line performance with different settling haxx
Didn't see much difference when changing the damping value from
like -200 to -1000
** DONE Nate dogg and Warren G had to S I M U L A T E
So its settling and driving straight in a circle.
Lets evaluate the untrained performance on the test data sets.
* DONE TRAIN NO WORKO
This is bad, because basically it's a brickwall if I can't get
around it somehow. I tried the most basic form of the problem.
I trained one parameter. The loss blew up and the param -> nan.
I trained one parameter and took an average over 10 trajectories. The loss blew up and the param -> nan.
I trained one parameter and took an average over 100 trajectories. The loss blew up and the param -> nan.
1 param, 100 traj, 2s traj, replace floats with double: param->nan
Traino, yes worko :)

So, this is not working because for some rare trajectories, the value of the gradient inexplicably explodes.
* DONE Exclude outlier gradient magnitudes
* DONE Running Loss? Didn't kill gradient explosions
* DONE Try smaller timestep? This actually seems like it works. WTF.
This seems to actually solve the problem fuck. But its too slow.
God damn it. Still some gradient explosions magnitude 1.
* DONE Identify the source of gradient explosions?
It could be that some part of it is not lipshitz, or it could
just be the general gradient variance problem that they talk
about in the paper "Gradients are not all you need"
It's caused by inverses, and division. Basically any
non-lipshitz component.
* DONE Adjust the small constant added to division?
in slip ratio and slip angle.
This fucking worked. It got rid of the 1e18 bullshit
but it still varies from 1e-6 to 1 which is atrocious
* New Network with non-lipshitz components eliminated?
Replace slip angle with Vy, replace slip ratio with vx - tire_tangent_vel
I'm not sure this would solve all the problems
* Colocation method (train derivatives)
Cheating. But simple and apparently works
Alternatively, just use very small trajectory length, I think.
I don't think colocation is going to work here because the real
data is too noisy. I would have to compute target derivatives
using finite differences which would be way too noisy.
* DONE Smaller duration trajectories
No Effect. Even with 2 points (smallest trajectory possible)
The gradient still explodes up to 1e18.
But now I can make a unit test to replicate this behavior and
find the source of it.

* DONE Euler vs RK4?
Idk why not.
Nope still explosive

* DONE Unit test to replicate exploding gradient
Able to replicate, I find it doesn't blow up out of nowhere,
it gradually blows up over a 100 steps.
Able to prevent the blow up by modifying the epsilon used to
avoid divide by zero when computing slip ratio.
Making the machine eps extremely small prevented any gradient
explosions when using train. This is great news. I am overjoyed.
Still getting gradient explosions, but much smaller magnitude.
~|1|
You could still just retrain the network to avoid dividing.

* DONE OH FUCK I WAS RETARDED AND MADE IT disCONTINUOUS OH SHIDDDDD
This will probably not solve the gradient-splosions.
Need to remove the discontinuity where Fz == 0 when zr < 0

* DONE Issues with ratio and diff networks
I noticed the original slip ratio network, occasionally has huge
.cpp training loss
Ratio network with 1e-12 epsilon is not settling correctly. (slip ratio explodes to 1e12)
Diff network has bad behavior (too much turning)
I fixed the ratio network by clamping the slip ratio.
Still seeing occasional massive gradient explosions. Usually at the
beginning of a test when tire velocities are zero
