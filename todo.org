Updates
* <2023-04-02 Sun 00:04>
I trained a new network, it uses relu, tanh, and sigmoid to make sure everything
crosses zero at zero and everything is continuous. Based.
I created unit tests to visualize the C++ implementation using matplotlib in cpp.
It matches the python plots, at least visually, I didn't actually compare the
numbers lol I just eyeballed it but it's probably good.
I'm surprised "eyeballing it" has an application in programming
* <2023-04-08 Sat 14:00>
I retrained a network to elimiate division.
So slip ratio is just velocity difference,
slip angle is just vy
It reaches the same level of training loss as with slip ratios.
More importantly, it explodes less. Only on a few rare occasions

does any element of the gradient exceed 1000.
Improved slip ratio network by making the epsilon much smaller
Also clamped the slip ratio.
This seems to mitigate most explosions. Only seems to explode at the
beginnings of a run when tire velocities are small

train.evaluate_cv3();
train.evaluate_ld3();

Before training:
CV3 avg loss: 0.0781642
LD3 avg loss: 0.0264296
After training:
CV3 avg loss: 0.0781142
LD3 avg loss: 0.0264126

Very minor improvement.
* <2023-04-09 Sun 16:02>
I retrained, using all available parameters and used RMSprop
instead of straight gradient descent.

Before training:
CV3 avg loss: 0.0781642
LD3 avg loss: 0.0264296
After Training:
CV3 avg loss: 0.0343975
LD3 avg loss: 0.015283

This improvement is Significant. We're so back.
* <2023-04-11 Tue 00:45>
I realized, the ode is still exploding with longer trajectories.
Explodes often with 4s and not at all with 2s.
I should stop fucking around.
Next step is to train with a bigger network, no bekker params,
and using the diff model. This has the highest chance to succeed.
I should run this shit and train the fuck out of it and then
move on with my god damn life.

Model is still training with 4s trajectories, but its slower and
its also exploding pretty often

* <2023-04-16 Sun 12:57>
Not able to get below 10% error.
I basically have no plan at this point, which is bad.
Using a smaller network. Only 8 hidden neurons.
I'm retraining with with 10 timesteps.
Then I'm going to retrain with 60 timesteps.
I'm desparate to get this bitch below 10%

A possible way forward: Make a 2d neural ode model.
Present results for both 3d and 2d models.
Use the 2d model to get the best possible results.
This is just cope. There's still no guarantee of reaching
a good accuracy level. It will maybe work.

Possibly remove the sign correction stuff and add a big
penalty to the loss function for an increasing system energy?
I think this is actually my best bet. I don't want to create a
fancy 2d model. That's pure copium.
Wait fuck, how do I create a loss function for the energy of the
system if the tires are adding energy?
Not sure. Maybe add a loss function to penalize the tire network
for violating energy?

Maybe differentiate through the vanilla bekker model to find
better tire-soil parameters? Have to cope with the slip ratios.
Numerical explosions likely. Bad.

Do I even need conservation of energy haxx now that I made
everything lipshitz continuous? Can I just slap my neural
network boy right in there and call it a day? Maybe I could
add an additional penalty to the loss function for when the
tire network doesn't cross zero at zero? Might work, who knows.

It's probably also worth a shot to just train the bekker params
with a neural tire-soil model.

1. Try to train without sign haxx
2. Try to train just no_slip model but just the bekker params

Currently training a small network at 10 timesteps, then I'm going
to scale it up to like 60 timesteps or more and see if the training
improves.

This is because I tried training with a bigger network and it
reached a worse asymptotic performance so maybe smaller is better.
Current validation loss (at 60 steps) is right about 10%
So maybe retraining over short trajectories with a smaller model was a good idea?
Cool, just realized I forgot to change the number of hidden units in TireNetwork.h
so it was reading in tons of uninitialized data. Wowee. Or was it loading a 20neuron
network from tire.net? World may never know tbh. Fuck. Well now it should train way faster
at least. Shit. Goind to make debug first this time.

Alright sick wow, now that I have this small network properly working it's giving me 10% error
out of the box with no fine tuning nice. Hopefully this shit works out this time.
*Withered wojak meme*
It was evaluating over 10 timesteps. It's over.
I fucked up again and it was evaling over 10 timesteps. Shit, fuck.
Okay, so now we are getting a relative error of 7.94%. Nice. That's enough.
After 1 training update, error blew back up to 15%. WTF
I believe this is the result of gradients that are too big
and I need to clip these gradients. If I wake up in the morning and
my shit is fucked, I'm going to compute stats on the gradients
and figure out a good clipping value.

Training over 10 steps worked great.
Training over 60 steps was terrible.
Validation Loss steadily increased the whole time from 15%->20%
Will try to compute gradient clipping values

Not working. loss is consistently rising. Fuck.

* <2023-04-18 Tue 18:03>
Will try training with no sign haxx and see what happens
Didn't work. Didn't settle. Basically that was expected.
Will try training the bekker params.
Also could try training the shit out of the model with
2 tiemsteps. Idk.

* <2023-04-19 Wed 17:58>
Training with bekker params straight up did not work.
I tried with 2 timesteps and it did much worse.
Maybe it will work for longer timesteps???
I thought I triwed that though?
Nope, training with bekker params is a failure rip.

* <2023-04-21 Fri 12:57>
Honestly training with 10 timesteps or 2 timesteps on the
small network is good enough performance. I think I will be
able to justify it, and make an argument about the difference between
training and test datasets.

* <2023-05-10 Wed 21:56>
Trying to improve performance on the ld3 test dataset.
I am trying to train with 60 steps and changing it from incrementing
by 60 to incrementing by 4 so effectively it trains over the
data many more times. Did not improve significantly.

justin@SenseNet:~/code/auvsl_dynamics_bptt/build$ grep "CV3 avg" train_output.txt
CV3 avg loss: 0.209038
CV3 avg loss: 0.149164
CV3 avg loss: 0.135871
CV3 avg loss: 0.131235
CV3 avg loss: 0.130213
CV3 avg loss: 0.128472
CV3 avg loss: 0.133146
CV3 avg loss: 0.133035
CV3 avg loss: 0.133578
CV3 avg loss: 0.135477
CV3 avg loss: 0.137866
CV3 avg loss: 0.139977
CV3 avg loss: 0.141098
justin@SenseNet:~/code/auvsl_dynamics_bptt/build$ grep "LD3 avg" train_output.txt
LD3 avg loss: 0.152006
LD3 avg loss: 0.151174
LD3 avg loss: 0.151254
LD3 avg loss: 0.151245
LD3 avg loss: 0.15123
LD3 avg loss: 0.151169
LD3 avg loss: 0.151135
LD3 avg loss: 0.151088
LD3 avg loss: 0.151048
LD3 avg loss: 0.151032
LD3 avg loss: 0.150993
LD3 avg loss: 0.150959
LD3 avg loss: 0.150932

* <2023-05-12 Fri 23:50>
Still trying to imrpvoe ld3 test dataset performance.
What if I tried training with timestep = 1e-4? Hmmm, idk lets see
And also, train_steps=2

I don't have any better ideas unfortunately.
LD3 performance is slowly decreasing but it might have
hit a wall. Not sure.
We now come to you live from the training process:
It appears to be slowing down and approaching about .15
Yeah it won't progress past .157. Fuck.

The only way forward that I can think of, is to add angular error
to the loss function. Done lets try it.
Need to fix the preprocessing script to bound yaw values from [-pi,pi]?
Also change loss function to get smallest angle between actual and gt

Adding angular error did not really improve the situation. It did improve
the CV3 accuracy even further but LD3 still craps out at like .155

* <2023-05-19 Fri 23:12>
Fixed possible typo in initializeState. It was:
  xk[14] = gt_state.vx;
  xk[15] = gt_state.vx;

So hopefully now that I changed that 2nd line to vy its
fixed. This should hopefully improve accuracy on LD3.
Spoiler: it didn't.

Another big error in how accuracy was computed.
loss = CppAD::Value(CppAD::sqrt(lin_mse / traj_len)); // (wrong)
vs
loss = CppAD::Value(CppAD::sqrt(lin_mse) / traj_len); // (correct)

This is big. This accounts for the discrepancy in LD3 vs CV3 performance.

CV3 Accuracy was reduced to 3.6%
LD3 Accuracy was reduced to 15.2%x

I don't know why.
I need to actually debug the LD3.
I'm not sure what is actually wrong.
I assumed it was bad longitudinal performance, but it could
literally be anything. I want to understand it it's yaw,x,or y
that is getting fucked up.

* <2023-05-20 Sat 11:45>
SHIT FUCK BITCH. LD3 is improved significantly by fixing these gay retarded errors, but now CV3 sucks.
LD3 avg loss: 0.0682702
CV3 avg loss: 0.140094

Need to evaluate CV3 test performance very carefully and diagnose errors.
It seems like yaw is mostly okay, I think. But longitudinal and lateral
performance is equally fucked. Maybe now that I have the validation
tests fixed, maybe a bigger network will help.

We could help identify possible areas for improvement by checking out the different CV3 trajectories.
There are some that are mostly straight, so we should be able to isolate the longitudinal performance.
Pretty sure.

Okay, so CV3 61 shows an example of us appearing to turn the wrong way.
There's some others. I want to check the initial vx, vy, wz

CV3 73 is pretty high speed (11mps) and the model goes straight
while gt turns. Pretty bad case. Could address this with a better
pretraining dataset.

CV3 98 is completely fucked. Looks like intial conditions are fucked.
CV3 104 is also completely fucked. Looks like intial conditions are fucked.

Im seeing a trend where it looks like for the second interval on each test
trajectory, the initial conditions look fucked.

So, now I'm going to train for a while. The performance should
improve a bit. Then I'm going to evaluate the same tests as above again.

Evaluating the same tests shows great results. Massive improvement.
No more glaring issues. Overall performance on these
4 CV3 tests is like 5.4%

With 32 hidden units:
LD3 avg loss: 1.03729
LD3 avg loss: 1.03623
LD3 avg loss: 0.756829
LD3 avg loss: 0.0846263
LD3 avg loss: 0.0759111
LD3 avg loss: 0.0672367
LD3 avg loss: 0.066986
LD3 avg loss: 0.0688053
LD3 avg loss: 0.0698828
LD3 avg loss: 0.0693427
LD3 avg loss: 0.0685603
LD3 avg loss: 0.0683913
LD3 avg loss: 0.068311

CV3 avg loss: 0.413945
CV3 avg loss: 0.412914
CV3 avg loss: 0.281909
CV3 avg loss: 0.165502
CV3 avg loss: 0.147556
CV3 avg loss: 0.126712
CV3 avg loss: 0.12138
CV3 avg loss: 0.120514
CV3 avg loss: 0.121548
CV3 avg loss: 0.12088
CV3 avg loss: 0.119026
CV3 avg loss: 0.118591
CV3 avg loss: 0.119088

It looks like now that the only main issue on CV3 is that
the relative performance is bad when the trajectory is short.
Rip.

Bad tests are CV3 #6,32,33,34,114 (there are more but hopefully
these are representative of any issues that are present)

Seems like the common thing between all these trajectories
is that the turns are sharp, with the left tires moving
around 0mps or less.

Not sure how to fix this stuff. Oh well.
I've been training with different number of steps.
I tried with 4 steps and it was basically
plataued on the same loss values.

Training now with 60 steps.
Tried with lr=1e-4 and validation loss was just
plateauing/changing very slowly.
Retrying with lr=1e-3 and m_cnt = 20
I can see the parameters are moving a bit more.

After a few hours, we got a 2 CV3 test results
CV3 avg loss: 0.117544
CV3 avg loss: 0.117482

Not significant.

* Ways forward from here (Good ideas are first):
** DONE Add wz to the tire network features.
I think this might improve rotation accuracy with sharp
rotation. This seems the most promising, tbh.
unfortunately, this would also require using a different network
for each tire. This is because, a positive Wz would result in
a different Fx and Fy for each tire. And each tire is getting
the abs of Vx and Vy so it has no idea how to correctly respond
to Wz.

Okay, I did this and got the error down to 10.3% for CV3
and 6.8% for LD3. Still not good enough.

Continued training at 10 steps.
CV3 error is down to 9.64%
ITS HAPPENING OH FUCK
Started training at 30 steps:
CV3 Error sort of stalled around 9.4%
Pretty lame

So now CV3 performance is reaching the linear model's level of
error. But the linear model's LD3 performance is 3.55%.
And our model has 6.3% error on LD3

Forward:
** DONE Add the tanh back and see how performance is affected.
Not as good.
Plateauing at 11.5% which sucks. Getting rid of tanh bought us
1.5% performance. I am literally killing myself for tiny
improvements.

** DONE Next, go up to 16 hidden units
Enough said. Didn't really help. Hmm.
** DONE Experiment with another network for predicting Fx based on Vx.
It has to be separate because the current network only takes diff
and this one needs to take Vx.
** DONE What if you got rid of zr (probably wont do this)
Enough said
CV3 avg loss: 0.1091
CV3 avg loss: 0.109098
CV3 avg loss: 0.109075
CV3 avg loss: 0.108971
CV3 avg loss: 0.108526
CV3 avg loss: 0.107223
CV3 avg loss: 0.105589
CV3 avg loss: 0.104644
CV3 avg loss: 0.104284
CV3 avg loss: 0.104204
CV3 avg loss: 0.104175
CV3 avg loss: 0.103811
CV3 avg loss: 0.103686
CV3 avg loss: 0.103726
CV3 avg loss: 0.10362
CV3 avg loss: 0.103338
CV3 avg loss: 0.103114
CV3 avg loss: 0.103125
CV3 avg loss: 0.103313
CV3 avg loss: 0.103355
CV3 avg loss: 0.103554
CV3 avg loss: 0.103843
CV3 avg loss: 0.104375
CV3 avg loss: 0.105802
CV3 avg loss: 0.107157
CV3 avg loss: 0.10913
CV3 avg loss: 0.110054
CV3 avg loss: 0.112307
This is fucking bullshit.

** (NAH) Experiment with another network for predicting Fx based on Vx.
It has to be separate because the current network only takes diff
and this one needs to take Vx.
** (STUPID) What if you got rid of zr (probably wont do this)
I don't think there is a significant variation in sinkage
for the jackal.
You could have one network that maps zr->sinkage
And then another network that does [Vx,Vy,Wz]->[Fx,Fy]

** DONE Go back to 8 hidden units
Check the accuracy. 32 hidden might not be necessary.
It's going a lot slower which is incovenient.

** DONE Remove the Tanh.
The tanh was used as a soft sign function. I'm not sure it was
a good idea. Using just diff or vy allows the Fx or Fy to
grow as diff or vy grow. Pretraining shows slightly better
accuracy too. Attempting this. right now.

Not much of an effect honestly.
CV3 avg loss: 0.120793
CV3 avg loss: 0.120793
CV3 avg loss: 0.121087

Maybe this would have gotten better if I let it run longer
but I don't think so.

I removed the tanh and went down to 8 hidden units.
We hit a wall at 12.97% CV3 error. So the 32 unit network gets
down to about 12%. Not great.


** DONE Maybe an atan based slip angle was necessary?
Slip angle changing depending on Vx might be necessary
to get better performance at low Vx, which would cause a higher
slip angle.
This slightly improved pretrain performance, but the scatter plots
still show that the error is worse when vx is low.
Not worth exploring imo.

** DONE Try training with Vx instead of diff?
Can check this with pretraining. This informs the network
directly about Vx which could address the above point as well.
Pretraining shows a slightly increased error level.
But this still might translate to a decreased validation test
level. Not really worth exploring.

Big brain thoughts incoming:
** DONE 4 networks, one for each tire.
Unlikely, but perhaps the added parameters will help.
And maybe there is a significant different between tires.
** DONE Only use zr and diff feature. Only return Fz,Fx
Fz is necesssary or we will sink through the floor.
But, the linear model just maps [vl,vr]->[vx,vy,wz]
So in my opinion, this shows that the model is almost completely
kinematic. So maybe simpler is better. Wait no, this is a dumb
idea. It would be able to slide laterally whicih would be bad.

** DONE Forward: modify loss function to include relative error
This will cause the loss function to focus on the small paths
and hopefully it will increase CV3 performance.
CV3 performance is at 8.66%
LD3 is around 6% so not great.
Continuiing training at 60 steps.


** TODO MULTITHREAD FINALLY
You've got 16 cores, use them all.

* A new way Forward: <2023-05-22 Mon 18:07>
Shit is enfuckulated.
I think you need to add the physical parameters to the model. Or, add another network
to apply an external force to the body of the vehicle to give us the extra params needed
to hack this fucking shit. I would prefer to use the physical params.

* DONE Rotate Initial quaternion according to yaw
* DONE Testing C++ code
* DONE Preprocess test data sets
* DONE settle. create initial position.
* DONE Create Unit tests
** unit test for settling, add a plot
** DONE Unit Test to confirm symmetry of the tire network
* DONE Train a New Network
Fuck. How should I architect this network.
Final Layer should be ReLU * Tanh(sign corection)
This enforces the basic rule of friction, that it opposes movement
* DONE Now that we have the network, S I M U L A T E
** DONE Create some unit tests
Create unit tests for basic simulations
Like moving forward along a straight line,
Moving in a circle
beautiful. So smooth and nice
** DONE Experiment with different settling damping hacks
Check the straight line performance with different settling haxx
Didn't see much difference when changing the damping value from
like -200 to -1000
** DONE Nate dogg and Warren G had to S I M U L A T E
So its settling and driving straight in a circle.
Lets evaluate the untrained performance on the test data sets.
* DONE TRAIN NO WORKO
This is bad, because basically it's a brickwall if I can't get
around it somehow. I tried the most basic form of the problem.
I trained one parameter. The loss blew up and the param -> nan.
I trained one parameter and took an average over 10 trajectories. The loss blew up and the param -> nan.
I trained one parameter and took an average over 100 trajectories. The loss blew up and the param -> nan.
1 param, 100 traj, 2s traj, replace floats with double: param->nan
Traino, yes worko :)

So, this is not working because for some rare trajectories, the value of the gradient inexplicably explodes.
* DONE Exclude outlier gradient magnitudes
* DONE Running Loss? Didn't kill gradient explosions
* DONE Try smaller timestep? This actually seems like it works. WTF.
This seems to actually solve the problem fuck. But its too slow.
God damn it. Still some gradient explosions magnitude 1.
* DONE Identify the source of gradient explosions?
It could be that some part of it is not lipshitz, or it could
just be the general gradient variance problem that they talk
about in the paper "Gradients are not all you need"
It's caused by inverses, and division. Basically any
non-lipshitz component.
* DONE Adjust the small constant added to division?
in slip ratio and slip angle.
This fucking worked. It got rid of the 1e18 bullshit
but it still varies from 1e-6 to 1 which is atrocious
* New Network with non-lipshitz components eliminated?
Replace slip angle with Vy, replace slip ratio with vx - tire_tangent_vel
I'm not sure this would solve all the problems
* Colocation method (train derivatives)
Cheating. But simple and apparently works
Alternatively, just use very small trajectory length, I think.
I don't think colocation is going to work here because the real
data is too noisy. I would have to compute target derivatives
using finite differences which would be way too noisy.
* DONE Smaller duration trajectories
No Effect. Even with 2 points (smallest trajectory possible)
The gradient still explodes up to 1e18.
But now I can make a unit test to replicate this behavior and
find the source of it.

* DONE Euler vs RK4?
Idk why not.
Nope still explosive

* DONE Unit test to replicate exploding gradient
Able to replicate, I find it doesn't blow up out of nowhere,
it gradually blows up over a 100 steps.
Able to prevent the blow up by modifying the epsilon used to
avoid divide by zero when computing slip ratio.
Making the machine eps extremely small prevented any gradient
explosions when using train. This is great news. I am overjoyed.
Still getting gradient explosions, but much smaller magnitude.
~|1|
You could still just retrain the network to avoid dividing.

* DONE OH FUCK I WAS RETARDED AND MADE IT disCONTINUOUS OH SHIDDDDD
This will probably not solve the gradient-splosions.
Need to remove the discontinuity where Fz == 0 when zr < 0

* DONE Issues with ratio and diff networks
I noticed the original slip ratio network, occasionally has huge
.cpp training loss
Ratio network with 1e-12 epsilon is not settling correctly. (slip ratio explodes to 1e12)
Diff network has bad behavior (too much turning)
I fixed the ratio network by clamping the slip ratio.
Still seeing occasional massive gradient explosions. Usually at the
beginning of a test when tire velocities are zero



* <2023-05-27 Sat 16:23>
Okay, the plan is to fix multi threading, then add another
network that applies an external force to the robot's base.

Currently, as far as multithreading goes, you've realized you just
need to make sure you copy m_params for each thread because
CppAD is too stupid to operate on the same CppAD::vector in
multiple threads.

Currently doing a test run with multithreading to make sure it
works and can reach the same performance that single threading
does.
Then: Remove quaternion initialization and fix the input scalers.

Okay multithreading completed. What else is needed for the final
run?

1. Multithreading
2. 2D training
3. No initial Quaternion
4. Base Link network.
   

** TODO Okay, but what if we fixed zr?
This dataset is 2d. Fz doesn't really matter, it just has to keep
us from sinking into the ground.

So, how much is the zr noise affecting training? Would it be better
to just ignore that and restrict motion to 2D during training?
And then do fine tuning pass where 3D motion is reactivated?

It would be easy to implement, why not try it?

Actually, why not train and evaluate in 2D?
The other models are 2D, so it would be a fair comparison.
I don't see why not.

** TODO Get rid of initial quaternion?
The initial quaternion determined by natural settling into the
ground. It has some close to zero pitch and roll but the yaw
component is .3 degrees. No bueno. Probably should delete.

** DONE THe fucking input scalers are biased
You took the scaled the absolute value of the inputs, not the actual
inputs. Fuck. This causes a non zero bias term. Shid. Will fix this
and see how much it actually affects performance. Pretraining shows
a small improvement in evaluation loss. OKay, yeah you will copy
that shit in.


* DONE Bugfix for computeEqState
You fucked up. The eq state is only initialized once in the
constructor. So if you intiialize your params to something
retarded, you will be stuck with a retarded initial state for the
rest of your training. Shit. This could have been having a small
effect on loss. or big idk. Actually this is not a big deal.
